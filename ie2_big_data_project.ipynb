{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IE2 Big Data Project (weilemar & vongrdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Jupyter Notebook wird der Code vorgestellt, welcher verwendet wurde um das [Homocide Report](https://www.kaggle.com/murderaccountability/homicide-reports) Dataset zu analysieren. Ziel dabei ist es ...\n",
    "\n",
    "Als erster Schritt muss das rohe Dataset über die folgende URL heruntergeladen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data\n",
    "wget -O homocide-reports.zip https://github.com/vongruenigen/IE2-Project/raw/master/data/homicide-reports.zip\n",
    "unzip -f homocide-reports.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Dataset heruntergeladen und entpackt ist muss daraus mittels des preprocessing Skripts (für Original siehe scripts/preprocess_data.py) das eigentliche Dataset mit One-Hot Encoded Vektoren verwendet werden. Zuerst aber einige Hilfsfunktionen, Import von Bibliotheken und Definitionen von Konstanten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from os import path\n",
    "\n",
    "# Constants for file names\n",
    "PROJECT_HOME = '/media/dvg/Volume/Dropbox/ZHAW/IE2/Project'\n",
    "RAW_DATA_PATH = 'data/database.csv'\n",
    "PREPROCESSED_DATA_PATH = 'data/samples.csv'\n",
    "\n",
    "# Hyperparameters for autoencoder\n",
    "TRAINING_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "DISPLAY_EPOCH = 1\n",
    "DISPLAY_BATCH = 1000\n",
    "HIDDEN_SIZE = 256\n",
    "RESULTS_DIR = path.abspath(path.join(PROJECT_HOME, 'results'))\n",
    "\n",
    "# Columns we are going to ignore in the dataset because they're redundant or non-informative\n",
    "STRIP_COLS = ('Record ID', 'Agency Name', 'Agency Code', 'Year', 'Month', 'Record Source')\n",
    "\n",
    "def camel_to_sneak(name):\n",
    "    '''Convert a string from camel-case to sneak-case.'''\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def error(msg):\n",
    "    '''Logs an error message and terminates the process.'''\n",
    "    log(msg, level='error')\n",
    "\n",
    "def log(msg, level='info'):\n",
    "    '''Logs the given message with the given level.'''\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    print('[%s][%s] %s' % (level.upper(), ts, msg))\n",
    "    if level == 'error': sys.exit(2)\n",
    "        \n",
    "\n",
    "def get_next_batch(train_data_f, batch_size):\n",
    "    '''Returns the next batch of vectors from the given CSV file.'''\n",
    "    new_batch = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        new_line = train_data_f.readline().strip('\\n')\n",
    "        new_batch.append(np.array((map(float, new_line.split(';')))))\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "def get_input_size_and_length(data_f):\n",
    "    input_size = len(data_f.readline().split(';'))\n",
    "    data_f.seek(0)\n",
    "\n",
    "    num_samples = sum([1 for _ in data_f])\n",
    "    data_f.seek(0)\n",
    "\n",
    "    return input_size, num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(RAW_DATA_PATH, 'r') as data_f:\n",
    "    with open(PREPROCESSED_DATA_PATH, 'w+') as out_f:\n",
    "        # Read headings\n",
    "        columns = data_f.readline().strip('\\n').split(',')\n",
    "        col_values = defaultdict(list)\n",
    "        last_idx = 0\n",
    "\n",
    "        log('The following columns are filtered: %s' % ', '.join(STRIP_COLS))\n",
    "\n",
    "        # Find all unique values for each row in the dataset\n",
    "        # and store them in col_values.\n",
    "        for i, line in enumerate(data_f):\n",
    "            sample_values = line.strip('\\n').split(',')\n",
    "\n",
    "            for c, v in zip(columns, sample_values):\n",
    "                if c in STRIP_COLS: continue\n",
    "                if v not in col_values[c]: col_values[c].append(v)\n",
    "\n",
    "            last_idx = i\n",
    "\n",
    "        log('The number of distinct values for each column are:\\n')\n",
    "\n",
    "        sum_lines = last_idx+1\n",
    "        sum_vec_entries = 0\n",
    "\n",
    "        for c, v in col_values.items():\n",
    "            log('  %s = %d' % (c, len(v)))\n",
    "            sum_vec_entries += len(v)\n",
    "\n",
    "        log('\\nThe generated vectors will have a total of %d entries each' % sum_vec_entries)\n",
    "        log('The dataset has %i samples' % sum_lines)\n",
    "\n",
    "        data_f.seek(0)\n",
    "        data_f.readline() # skip headings after seek(0)\n",
    "\n",
    "        start_time = time.time()\n",
    "        curr_idx = 0\n",
    "        temp_x = []\n",
    "\n",
    "        for i, line in enumerate(data_f):\n",
    "            sample_values = line.strip('\\n').split(',')\n",
    "            sample_vec = np.zeros(sum_vec_entries)\n",
    "            idx_offset = 0\n",
    "\n",
    "            for c, v in zip(columns, sample_values):\n",
    "                if c in STRIP_COLS: continue\n",
    "                sample_vec[col_values[c].index(v)+idx_offset] = 1\n",
    "                idx_offset += len(col_values[c])\n",
    "\n",
    "            temp_x.append(sample_vec)\n",
    "\n",
    "            if (i+1) % 100000 == 0 or (i+1) == sum_lines:\n",
    "                temp_x = np.array(temp_x)\n",
    "                np.random.shuffle(temp_x)\n",
    "\n",
    "                log('Processed %i samples (%.1f%%)...' % (i+1, 100*(float(i+1)/sum_lines)))\n",
    "                log('Storing collected data in CSV file...')\n",
    "\n",
    "                temp_x_str = []\n",
    "\n",
    "                for i in range(temp_x.shape[0]):\n",
    "                    temp_x_str.append(';'.join(map(str, map(int, temp_x[i]))))\n",
    "           \n",
    "                out_f.write('%s\\n' % '\\n'.join(temp_x_str))\n",
    "\n",
    "                curr_idx += temp_x.shape[0]\n",
    "\n",
    "                log('Stored data successfully! (Took %.2fs)' % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                temp_x = []\n",
    "\n",
    "        log('Successfully stored preprocessed samples in: %s' % PREPROCESSED_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Skript konvertiert alle Morde in _database.csv_ in One-Hot Encoded Vektoren und speichert diese in einer CSV Datei. Diese können dann verwendet werden um damit den _AutEncoder_ zu trainieren und danach Embeddings für jeden einzelnen Mord zu generieren.\n",
    "\n",
    "Beim Preprocessen werden alle Spalten berücksichtigt ausser diejenigen, welche in der Liste STRIP_COLS explizit ausgeschlossen werden. Das ganz funktioniert so, dass zuerst für jede Spalte eruiert wird, wieviele unterschiedliche Werte es pro Spalte hat. Wenn für eine Spalte _n_ verschiedene Werte vorhanden sind, so werden für die Darstellung im One-Hot Vektor entsprechend _n_ Werte für diese Spalte benötigt. Die resultierenden Vektoren sind also \n",
    "\n",
    "$$\\sum_{c \\in Columns} \\operatorname{classes}(c)$$\n",
    "\n",
    "lang, wobei _C_ für die Menge aller Spalten und _classes_ für die Anzahl unterschiedlicher Werte für die Spalte _c_ steht. Pro Zeile und Spalte werden dann diejenigen Werte, welche in der jeweiligen Zeile stehen auf _1_ gesetzt, alle anderen werden auf _0_ belassen. Als nächste folgt der Code in [TensorFlow](https://www.tensorflow.org/), welcher für die Implementation des _AutoEncoder_ und des _VariationalAutoEncoder_ zuständig ist.\n",
    "\n",
    "Prinzipell sind _AutoEncoder_ eine spezielle Art von Neuronalen Netzen (NN), welche dafür zuständig sind, eine effiziente Codierung der Eingabedaten zu lernen. Der Aufbau ist so, dass die Eingabedaten als One-Hote Encoded Vektoren (im Bild unten _x_) über den Input Layer in das NN eingespeist wird. Diese werden dann mithilfe einer Multiplikation mit einer Gewichts-Matrix in den Hidden Layer projeziert. Dieser ist im Falle von AutoEncodern **immer** kleiner wie der Input Layer, weil das NN ja eine effiziente Codierung der Eingabedaten lernen soll. Am Ende wird das NN mithilfe von Gradient-Descent mit dem _Adam_ Optimierer darauf trainiert, aus der codierten Darstellung der Eingabedaten (im Bild unten _z_) wieder die Eingabedaten _x_ zu rekonstruieren. Die generierten Darstellungen _z_ können dann als Embeddings der Eingabedaten in einem _m_-dimensionalen Vektorraum aufgefasst werden, wobei _m_ der grösse des Hidden Layer in der Mitte entspricht. Auf diese Embeddings können wir dann später Clustering-Algorithmen anwenden um festzustellen, welche Verbrechen im eingebetteten Vektor-Raum nahe beieinander liegen.\n",
    "\n",
    "![AutoEncoder Struktur](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n",
    "\n",
    "Der Unterschied eines \"normalen\" _AutoEncoder_ zu einem _VariationalAutoEncoder_ liegt darin, dass ...\n",
    "Für eine gute Einführung in _VariationalAutoEncoder_ kann [dieses](http://kvfrans.com/variational-autoencoders-explained/) Tutorial hinzugezogen werden.\n",
    "\n",
    "Wir werden alle Experimente mit beiden Varianten _AutoEncoder_ und _VariationalAutoEncoder_ durchführen. Unten folgt die Defintion der Modelle in Python mithilfe von TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The current version of the autoencoder to use\n",
    "CurrentAutoEncoder = AutoEncoder\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, input_size, hidden_size, session):\n",
    "        '''Initializes a new instance of the VariationalAutoencoder class.'''\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {}\n",
    "        self.transfer_fn = tf.nn.softplus\n",
    "\n",
    "        self.__initialize()\n",
    "        self.__build()\n",
    "\n",
    "    def __initialize(self):\n",
    "        '''Initializes the weights needed to build the computational graph.'''\n",
    "        weights = {}\n",
    "\n",
    "        weights['weights_1'] = tf.get_variable('weights_1',[self.input_size, self.hidden_size])\n",
    "        weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['weights_2'] = tf.Variable(tf.zeros([self.hidden_size, self.input_size], dtype=tf.float32))\n",
    "        weights['bias_2'] = tf.Variable(tf.zeros([self.input_size], dtype=tf.float32))\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''Returns the optimizer for this instance.'''\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_loss(self):\n",
    "        '''Returns the loss function for this instance.'''\n",
    "        return self.loss_fn\n",
    "\n",
    "    def get_weights_and_biases(self):\n",
    "        '''Returns the weights and biases of this instance.'''\n",
    "        return self.weights\n",
    "\n",
    "    def get_internal_representation(self):\n",
    "        '''Returns the internal, embedded representation variables.'''\n",
    "        return self.hidden\n",
    "\n",
    "    def batch_fit(self, input):\n",
    "        '''Fits the model to the given batch.'''\n",
    "        loss, _ = self.session.run((self.loss_fn, self.optimizer),\n",
    "                                   feed_dict={self.input: input})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, input):\n",
    "        return self.session.run(self.hidden, feed_dict={self.input: input})\n",
    "\n",
    "    def __build(self):\n",
    "        '''Builds the computational graph.'''\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        hidden_1_result = tf.matmul(self.input, self.weights['weights_1'])\n",
    "        self.hidden = self.transfer_fn(tf.add(hidden_1_result,\n",
    "                                              self.weights['bias_1']))\n",
    "\n",
    "        reconstruction_result = tf.matmul(self.hidden, self.weights['weights_2'])\n",
    "        self.reconstruction = tf.add(reconstruction_result, self.weights['bias_2'])\n",
    "\n",
    "        diff = tf.subtract(self.reconstruction, self.input)\n",
    "        self.loss_fn = 0.5 * tf.reduce_sum(tf.pow(diff, 2.0))\n",
    "        self.optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.optimizer = self.optimizer_fn.minimize(self.loss_fn)\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, input_size, hidden_size, session):\n",
    "        '''Initializes a new instance of the VariationalAutoencoder class.'''\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {}\n",
    "\n",
    "        self.__initialize()\n",
    "        self.__build()\n",
    "\n",
    "    def __initialize(self):\n",
    "        '''Initializes the weights needed to build the computational graph.'''\n",
    "        weights = {}\n",
    "\n",
    "        weights['weights_1'] = tf.get_variable('weights_1',[self.input_size, self.hidden_size])\n",
    "        weights['log_sigma_weights_1'] = tf.get_variable('log_sigma_weights_1', [self.input_size, self.hidden_size])\n",
    "        weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['log_sigma_bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['weights_2'] = tf.Variable(tf.zeros([self.hidden_size, self.input_size], dtype=tf.float32))\n",
    "        weights['bias_2'] = tf.Variable(tf.zeros([self.input_size], dtype=tf.float32))\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''Returns the optimizer for this instance.'''\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_loss(self):\n",
    "        '''Returns the loss function for this instance.'''\n",
    "        return self.loss_fn\n",
    "\n",
    "    def get_weights(self):\n",
    "        '''Returns the weights of this instance.'''\n",
    "        return self.weights\n",
    "\n",
    "    def get_internal_representation(self):\n",
    "        '''Returns the internal, embedded representation variables.'''\n",
    "        return self.z\n",
    "\n",
    "    def batch_fit(self, input):\n",
    "        '''Fits the model to the given batch.'''\n",
    "        loss, _ = self.session.run((self.loss_fn, self.optimizer),\n",
    "                                   feed_dict={self.input: input})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, input):\n",
    "        return self.session.run(self.z_mean, feed_dict={self.input: input})\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        loss, opt = self.sess.run((self.loss_fn, self.optimizer), feed_dict={self.x: X})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "\n",
    "    def __build(self):\n",
    "        '''Builds the computational graph.'''\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        hidden_1_result = tf.matmul(self.input, self.weights['weights_1'])\n",
    "        self.z_mean = tf.add(hidden_1_result, self.weights['log_sigma_bias_1'])\n",
    "\n",
    "        log_sigma_result = tf.matmul(self.input, self.weights['log_sigma_weights_1'])\n",
    "        self.z_log_sigma_sq = tf.add(log_sigma_result, self.weights['log_sigma_bias_1'])\n",
    "\n",
    "        eps = tf.random_normal(tf.stack([tf.shape(self.input)[0], self.hidden_size]), 0, 1, dtype=tf.float32)\n",
    "        self.z = tf.add(self.z_mean, tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        y_result = tf.matmul(self.z, self.weights['weights_2'])\n",
    "        self.y = tf.add(y_result, self.weights['bias_2'])\n",
    "\n",
    "        reconstruction_loss = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.y, self.input), 2.0))\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \\\n",
    "                                           - tf.square(self.z_mean) \\\n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "        self.loss_fn = tf.reduce_mean(reconstruction_loss + latent_loss)\n",
    "        self.optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.optimizer = self.optimizer_fn.minimize(self.loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Definition der Modelle folgt der Code um diese zu trainieren. Das Training durchzuführen dauert entsprechend lange, auf einer mittelmässigen GPU benötigt es ca. einen Tag Rechenzeit. Im folgenden Abschnitt ist der Code, welcher zuständig ist für das Training des _AutoEncoder_, ersichtlich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_type = camel_to_sneak(CurrentAutoEncoder.__name__)\n",
    "time_stamp = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())\n",
    "result_name = '%s-%s-results/' % (time_stamp, encoder_type)\n",
    "result_path = path.join(RESULTS_DIR, result_name)\n",
    "loss_track = []\n",
    "\n",
    "with open(PREPROCESSED_DATA_PATH, 'r') as train_f:\n",
    "    input_size, num_samples = get_input_size_and_length(train_f)\n",
    "\n",
    "    log('Starting training with a %s' % CurrentAutoEncoder.__name__)\n",
    "\n",
    "    autoencoder = CurrentAutoEncoder(input_size, HIDDEN_SIZE, session=session)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        log('Starting epoch #%d' % (epoch+1))\n",
    "        num_batches = int(num_samples / BATCH_SIZE)\n",
    "        avg_loss = 0\n",
    "\n",
    "        for num_batch in range(num_batches):\n",
    "            batch_x = get_next_batch(train_f, BATCH_SIZE)\n",
    "            loss = autoencoder.batch_fit(batch_x)\n",
    "            avg_loss += (loss / num_samples) * BATCH_SIZE\n",
    "\n",
    "            if (num_batch+1) % DISPLAY_BATCH == 0 or (num_batches-num_batch) < 5:\n",
    "                log('Batch #%d of #%d, loss = %.5f' % (num_batch+1, num_batches, loss))\n",
    "\n",
    "        if (epoch+1) % DISPLAY_EPOCH == 0 or (epoch+1) == TRAINING_EPOCHS:\n",
    "            log('Epoch #%d of #%d, loss = %.5f' % (epoch+1, TRAINING_EPOCHS, avg_loss))\n",
    "            saver.save(session, result_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem der _AutoEncoder_ trainiert wurde kann dieser verwendet werden um damit die Embeddings für die einzelnen Verbrechen zu generieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PREPROCESSED_DATA_PATH, 'r') as samples_f:\n",
    "    with open(emb_out_path, 'w+') as emb_f:\n",
    "        input_size, num_samples = get_input_size_and_length(train_f)\n",
    "        log('Restoring model from %s' % model_path)\n",
    "\n",
    "        autoencoder = CurrentAutoEncoder(input_size, HIDDEN_SIZE, session=session)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "        saver.restore(session, model_path)\n",
    "\n",
    "        log('Finished restoring the model')\n",
    "        log('Starting to embed the samples in %s' % samples_path)\n",
    "\n",
    "        num_batches = int(num_samples / BATCH_SIZE)\n",
    "\n",
    "        for num_batch in range(num_batches):\n",
    "            batch_x = get_next_batch(samples_data, num_batch)\n",
    "            batch_y = autoencoder.transform(batch_x)\n",
    "\n",
    "            for y in batch_y:\n",
    "                emb_f.write('%s\\n' % ';'.join(map(str, y)))\n",
    "\n",
    "            if (num_batch+1) % DISPLAY_BATCH == 0:\n",
    "                log('Processed %d of %d samples' % (num_batch+1, num_batches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun, da wir die Embeddings für alle Verbrechen generiert haben, können wir damit starten das K-Means Clustering auf diese anzuwenden, als auch auf die ursprünglichen One-Hot Encoded Vektoren, welche verwendet wurden um den _AutoEncoder_ zu trainieren:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
