{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IE2 Big Data Project (weilemar & vongrdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Jupyter Notebook wird der Code vorgestellt, welcher verwendet wurde um das [Homocide Report](https://www.kaggle.com/murderaccountability/homicide-reports) Dataset zu analysieren. Der Auftraggeber auf Kaggle ist die Organisation Murder Accountability Project (http://www.murderdata.org/).\n",
    "In diesem Datensatz befinden sich 23 Attribute für 638‘454 Morde in Amerika von 1980-2014. Ziel des Kaggle Aufrages ist es, ein Muster in den Daten für \"Serial-Killer\" zu entdecken. Mit diesen Daten könnte man auch sehr viele, spannende Diagramme erstellen. Jedoch liegt unser Fokus in dieser Arbeit auf dem clustern von Daten.\n",
    "Konkret möchten wir mit 2 verschiednen Daten Cluster mit k-means erstellen. Einerseits verwenden wir die Rohdaten in One-hot-encoding Form. Andererseits verwenden wir Embeddings welche mit Hilfe unserers zuvor trainierten Autoencoder generiert werden. Wir möchten die resultierenden Cluster vergleichen und auf beiden Resultaten anschliessend noch mit Hilfe einer PCA herausfinden, welche Attribute ausschlaggebend waren.\n",
    "\n",
    "Als erster Schritt muss das rohe Dataset über die folgende URL heruntergeladen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  homocide-reports.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2017-05-26 16:42:02--  https://github.com/vongruenigen/IE2-Project/raw/master/data/homicide-reports.zip\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/vongruenigen/IE2-Project/master/data/homicide-reports.zip [following]\n",
      "--2017-05-26 16:42:03--  https://raw.githubusercontent.com/vongruenigen/IE2-Project/master/data/homicide-reports.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.36.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.36.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11010328 (10M) [application/octet-stream]\n",
      "Saving to: ‘homocide-reports.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  811K 13s\n",
      "    50K .......... .......... .......... .......... ..........  0% 1.88M 9s\n",
      "   100K .......... .......... .......... .......... ..........  1% 7.16M 7s\n",
      "   150K .......... .......... .......... .......... ..........  1% 2.82M 6s\n",
      "   200K .......... .......... .......... .......... ..........  2% 8.21M 5s\n",
      "   250K .......... .......... .......... .......... ..........  2% 9.34M 4s\n",
      "   300K .......... .......... .......... .......... ..........  3% 8.83M 4s\n",
      "   350K .......... .......... .......... .......... ..........  3% 12.9M 3s\n",
      "   400K .......... .......... .......... .......... ..........  4% 3.75M 3s\n",
      "   450K .......... .......... .......... .......... ..........  4% 11.9M 3s\n",
      "   500K .......... .......... .......... .......... ..........  5% 11.4M 3s\n",
      "   550K .......... .......... .......... .......... ..........  5% 9.26M 3s\n",
      "   600K .......... .......... .......... .......... ..........  6% 10.5M 3s\n",
      "   650K .......... .......... .......... .......... ..........  6% 1.14M 3s\n",
      "   700K .......... .......... .......... .......... ..........  6% 28.1M 3s\n",
      "   750K .......... .......... .......... .......... ..........  7% 21.5M 3s\n",
      "   800K .......... .......... .......... .......... ..........  7% 50.8M 2s\n",
      "   850K .......... .......... .......... .......... ..........  8% 96.2M 2s\n",
      "   900K .......... .......... .......... .......... ..........  8%  171M 2s\n",
      "   950K .......... .......... .......... .......... ..........  9% 6.20M 2s\n",
      "  1000K .......... .......... .......... .......... ..........  9% 13.2M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 10% 16.8M 2s\n",
      "  1100K .......... .......... .......... .......... .......... 10% 13.6M 2s\n",
      "  1150K .......... .......... .......... .......... .......... 11% 9.71M 2s\n",
      "  1200K .......... .......... .......... .......... .......... 11% 44.3M 2s\n",
      "  1250K .......... .......... .......... .......... .......... 12% 18.5M 2s\n",
      "  1300K .......... .......... .......... .......... .......... 12% 18.0M 2s\n",
      "  1350K .......... .......... .......... .......... .......... 13%  118M 2s\n",
      "  1400K .......... .......... .......... .......... .......... 13% 58.5M 2s\n",
      "  1450K .......... .......... .......... .......... .......... 13% 10.1M 2s\n",
      "  1500K .......... .......... .......... .......... .......... 14% 1.17M 2s\n",
      "  1550K .......... .......... .......... .......... .......... 14% 18.1M 2s\n",
      "  1600K .......... .......... .......... .......... .......... 15%  129M 2s\n",
      "  1650K .......... .......... .......... .......... .......... 15%  154M 2s\n",
      "  1700K .......... .......... .......... .......... .......... 16%  155M 2s\n",
      "  1750K .......... .......... .......... .......... .......... 16%  154M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 17%  134M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 17%  157M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 18%  130M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 18%  128M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 19% 14.3M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 19% 6.92M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 19% 3.44M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 20% 8.46M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 20% 10.5M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 21% 6.53M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 21% 12.1M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 22% 12.0M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 22% 9.04M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 23% 6.34M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 23% 12.3M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 24% 11.6M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 24% 10.8M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 25% 11.4M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 25% 12.4M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 26% 8.67M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 26% 9.03M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 26% 10.1M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 27% 11.3M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 27% 6.24M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 28% 10.8M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 28% 12.3M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 29% 11.1M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 29% 11.6M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 30% 6.54M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 30% 9.33M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 31% 12.4M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 31% 7.42M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 32% 11.0M 1s\n",
      "  3450K .......... .......... .......... .......... .......... 32% 12.1M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 33% 7.18M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 33% 9.04M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 33% 8.88M 1s\n",
      "  3650K .......... .......... .......... .......... .......... 34% 11.1M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 34% 11.8M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 35% 7.20M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 35% 7.91M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 36% 11.1M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 36% 12.1M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 37% 9.89M 1s\n",
      "  4000K .......... .......... .......... .......... .......... 37% 6.46M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 38% 8.22M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 38% 8.84M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 39% 11.8M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 39% 11.3M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 39% 4.69M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 40% 8.89M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 40% 8.81M 1s\n",
      "  4400K .......... .......... .......... .......... .......... 41% 8.79M 1s\n",
      "  4450K .......... .......... .......... .......... .......... 41% 12.2M 1s\n",
      "  4500K .......... .......... .......... .......... .......... 42% 8.40M 1s\n",
      "  4550K .......... .......... .......... .......... .......... 42% 8.61M 1s\n",
      "  4600K .......... .......... .......... .......... .......... 43% 2.52M 1s\n",
      "  4650K .......... .......... .......... .......... .......... 43% 12.4M 1s\n",
      "  4700K .......... .......... .......... .......... .......... 44% 9.87M 1s\n",
      "  4750K .......... .......... .......... .......... .......... 44% 6.50M 1s\n",
      "  4800K .......... .......... .......... .......... .......... 45% 2.58M 1s\n",
      "  4850K .......... .......... .......... .......... .......... 45% 9.66M 1s\n",
      "  4900K .......... .......... .......... .......... .......... 46% 3.73M 1s\n",
      "  4950K .......... .......... .......... .......... .......... 46% 7.41M 1s\n",
      "  5000K .......... .......... .......... .......... .......... 46% 7.10M 1s\n",
      "  5050K .......... .......... .......... .......... .......... 47% 3.67M 1s\n",
      "  5100K .......... .......... .......... .......... .......... 47% 5.05M 1s\n",
      "  5150K .......... .......... .......... .......... .......... 48% 3.29M 1s\n",
      "  5200K .......... .......... .......... .......... .......... 48% 2.20M 1s\n",
      "  5250K .......... .......... .......... .......... .......... 49% 11.0M 1s\n",
      "  5300K .......... .......... .......... .......... .......... 49%  945K 1s\n",
      "  5350K .......... .......... .......... .......... .......... 50% 20.8M 1s\n",
      "  5400K .......... .......... .......... .......... .......... 50%  124M 1s\n",
      "  5450K .......... .......... .......... .......... .......... 51%  153M 1s\n",
      "  5500K .......... .......... .......... .......... .......... 51%  146M 1s\n",
      "  5550K .......... .......... .......... .......... .......... 52%  158M 1s\n",
      "  5600K .......... .......... .......... .......... .......... 52%  141M 1s\n",
      "  5650K .......... .......... .......... .......... .......... 53% 6.93M 1s\n",
      "  5700K .......... .......... .......... .......... .......... 53% 6.78M 1s\n",
      "  5750K .......... .......... .......... .......... .......... 53% 2.67M 1s\n",
      "  5800K .......... .......... .......... .......... .......... 54% 6.69M 1s\n",
      "  5850K .......... .......... .......... .......... .......... 54% 6.75M 1s\n",
      "  5900K .......... .......... .......... .......... .......... 55% 5.98M 1s\n",
      "  5950K .......... .......... .......... .......... .......... 55% 6.50M 1s\n",
      "  6000K .......... .......... .......... .......... .......... 56% 3.92M 1s\n",
      "  6050K .......... .......... .......... .......... .......... 56% 7.81M 1s\n",
      "  6100K .......... .......... .......... .......... .......... 57% 5.29M 1s\n",
      "  6150K .......... .......... .......... .......... .......... 57% 10.3M 1s\n",
      "  6200K .......... .......... .......... .......... .......... 58% 6.61M 1s\n",
      "  6250K .......... .......... .......... .......... .......... 58% 3.00M 1s\n",
      "  6300K .......... .......... .......... .......... .......... 59% 2.58M 1s\n",
      "  6350K .......... .......... .......... .......... .......... 59% 7.79M 1s\n",
      "  6400K .......... .......... .......... .......... .......... 59% 2.83M 1s\n",
      "  6450K .......... .......... .......... .......... .......... 60% 6.85M 1s\n",
      "  6500K .......... .......... .......... .......... .......... 60% 3.84M 1s\n",
      "  6550K .......... .......... .......... .......... .......... 61% 6.97M 1s\n",
      "  6600K .......... .......... .......... .......... .......... 61% 11.1M 1s\n",
      "  6650K .......... .......... .......... .......... .......... 62% 2.84M 1s\n",
      "  6700K .......... .......... .......... .......... .......... 62% 3.94M 1s\n",
      "  6750K .......... .......... .......... .......... .......... 63% 4.66M 1s\n",
      "  6800K .......... .......... .......... .......... .......... 63% 5.38M 1s\n",
      "  6850K .......... .......... .......... .......... .......... 64% 9.91M 1s\n",
      "  6900K .......... .......... .......... .......... .......... 64% 4.33M 1s\n",
      "  6950K .......... .......... .......... .......... .......... 65% 5.03M 1s\n",
      "  7000K .......... .......... .......... .......... .......... 65% 3.58M 1s\n",
      "  7050K .......... .......... .......... .......... .......... 66% 3.19M 1s\n",
      "  7100K .......... .......... .......... .......... .......... 66% 5.25M 1s\n",
      "  7150K .......... .......... .......... .......... .......... 66% 6.11M 1s\n",
      "  7200K .......... .......... .......... .......... .......... 67% 4.19M 1s\n",
      "  7250K .......... .......... .......... .......... .......... 67% 11.8M 1s\n",
      "  7300K .......... .......... .......... .......... .......... 68% 7.43M 1s\n",
      "  7350K .......... .......... .......... .......... .......... 68% 5.35M 1s\n",
      "  7400K .......... .......... .......... .......... .......... 69% 5.57M 1s\n",
      "  7450K .......... .......... .......... .......... .......... 69% 2.37M 1s\n",
      "  7500K .......... .......... .......... .......... .......... 70% 11.7M 0s\n",
      "  7550K .......... .......... .......... .......... .......... 70% 6.24M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 71% 4.91M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 71% 5.76M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 72% 9.72M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 72% 4.87M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 73% 5.68M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 73% 4.13M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 73% 11.2M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 74% 6.65M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 74% 5.57M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 75% 4.09M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 75% 6.32M 0s\n",
      "  8150K .......... .......... .......... .......... .......... 76% 3.73M 0s\n",
      "  8200K .......... .......... .......... .......... .......... 76% 6.47M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 77% 5.84M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 77% 11.8M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 78% 9.87M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 78% 3.52M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 79% 3.62M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 79% 9.12M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 79% 9.22M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 80% 7.19M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 80% 8.91M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 81% 5.39M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 81% 7.73M 0s\n",
      "  8800K .......... .......... .......... .......... .......... 82% 4.17M 0s\n",
      "  8850K .......... .......... .......... .......... .......... 82% 5.72M 0s\n",
      "  8900K .......... .......... .......... .......... .......... 83% 4.99M 0s\n",
      "  8950K .......... .......... .......... .......... .......... 83% 8.56M 0s\n",
      "  9000K .......... .......... .......... .......... .......... 84% 7.59M 0s\n",
      "  9050K .......... .......... .......... .......... .......... 84% 6.62M 0s\n",
      "  9100K .......... .......... .......... .......... .......... 85% 7.26M 0s\n",
      "  9150K .......... .......... .......... .......... .......... 85% 6.20M 0s\n",
      "  9200K .......... .......... .......... .......... .......... 86% 5.59M 0s\n",
      "  9250K .......... .......... .......... .......... .......... 86% 7.12M 0s\n",
      "  9300K .......... .......... .......... .......... .......... 86% 3.68M 0s\n",
      "  9350K .......... .......... .......... .......... .......... 87% 8.52M 0s\n",
      "  9400K .......... .......... .......... .......... .......... 87% 6.84M 0s\n",
      "  9450K .......... .......... .......... .......... .......... 88% 9.32M 0s\n",
      "  9500K .......... .......... .......... .......... .......... 88% 11.6M 0s\n",
      "  9550K .......... .......... .......... .......... .......... 89% 5.73M 0s\n",
      "  9600K .......... .......... .......... .......... .......... 89% 5.63M 0s\n",
      "  9650K .......... .......... .......... .......... .......... 90% 7.90M 0s\n",
      "  9700K .......... .......... .......... .......... .......... 90% 7.89M 0s\n",
      "  9750K .......... .......... .......... .......... .......... 91% 7.20M 0s\n",
      "  9800K .......... .......... .......... .......... .......... 91% 8.48M 0s\n",
      "  9850K .......... .......... .......... .......... .......... 92% 9.65M 0s\n",
      "  9900K .......... .......... .......... .......... .......... 92% 8.56M 0s\n",
      "  9950K .......... .......... .......... .......... .......... 93% 7.82M 0s\n",
      " 10000K .......... .......... .......... .......... .......... 93% 6.93M 0s\n",
      " 10050K .......... .......... .......... .......... .......... 93% 10.2M 0s\n",
      " 10100K .......... .......... .......... .......... .......... 94% 8.93M 0s\n",
      " 10150K .......... .......... .......... .......... .......... 94% 9.47M 0s\n",
      " 10200K .......... .......... .......... .......... .......... 95% 8.10M 0s\n",
      " 10250K .......... .......... .......... .......... .......... 95% 11.3M 0s\n",
      " 10300K .......... .......... .......... .......... .......... 96% 9.85M 0s\n",
      " 10350K .......... .......... .......... .......... .......... 96% 8.41M 0s\n",
      " 10400K .......... .......... .......... .......... .......... 97% 6.88M 0s\n",
      " 10450K .......... .......... .......... .......... .......... 97% 6.92M 0s\n",
      " 10500K .......... .......... .......... .......... .......... 98% 10.3M 0s\n",
      " 10550K .......... .......... .......... .......... .......... 98% 4.66M 0s\n",
      " 10600K .......... .......... .......... .......... .......... 99% 11.8M 0s\n",
      " 10650K .......... .......... .......... .......... .......... 99% 8.05M 0s\n",
      " 10700K .......... .......... .......... .......... .......... 99% 9.47M 0s\n",
      " 10750K ..                                                    100% 18.0M=1.6s\n",
      "\n",
      "2017-05-26 16:42:04 (6.43 MB/s) - ‘homocide-reports.zip’ saved [11010328/11010328]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd data\n",
    "wget -O homocide-reports.zip https://github.com/vongruenigen/IE2-Project/raw/master/data/homicide-reports.zip\n",
    "unzip -f homocide-reports.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Dataset heruntergeladen und entpackt ist muss daraus mittels des preprocessing Skripts (für Original siehe scripts/preprocess_data.py) das eigentliche Dataset mit One-Hot Encoded Vektoren verwendet werden. Zuerst aber einige Hilfsfunktionen, Import von Bibliotheken und Definitionen von Konstanten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from os import path\n",
    "\n",
    "# Constants for file names\n",
    "PROJECT_HOME = '/media/dvg/Volume/Dropbox/ZHAW/IE2/Project'\n",
    "RAW_DATA_PATH = 'data/database.csv'\n",
    "PREPROCESSED_DATA_PATH = 'data/samples.csv'\n",
    "\n",
    "# Hyperparameters for autoencoder\n",
    "TRAINING_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "DISPLAY_EPOCH = 1\n",
    "DISPLAY_BATCH = 1000\n",
    "HIDDEN_SIZE = 256\n",
    "RESULTS_DIR = path.abspath(path.join(PROJECT_HOME, 'results'))\n",
    "\n",
    "# Columns we are going to ignore in the dataset because they're redundant or non-informative\n",
    "STRIP_COLS = ('Record ID', 'Agency Name', 'Agency Code', 'Year', 'Month', 'Record Source')\n",
    "\n",
    "def camel_to_sneak(name):\n",
    "    '''Convert a string from camel-case to sneak-case.'''\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def error(msg):\n",
    "    '''Logs an error message and terminates the process.'''\n",
    "    log(msg, level='error')\n",
    "\n",
    "def log(msg, level='info'):\n",
    "    '''Logs the given message with the given level.'''\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    print('[%s][%s] %s' % (level.upper(), ts, msg))\n",
    "    if level == 'error': sys.exit(2)\n",
    "        \n",
    "\n",
    "def get_next_batch(train_data_f, batch_size):\n",
    "    '''Returns the next batch of vectors from the given CSV file.'''\n",
    "    new_batch = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        new_line = train_data_f.readline().strip('\\n')\n",
    "        new_batch.append(np.array((map(float, new_line.split(';')))))\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "def get_input_size_and_length(data_f):\n",
    "    input_size = len(data_f.readline().split(';'))\n",
    "    data_f.seek(0)\n",
    "\n",
    "    num_samples = sum([1 for _ in data_f])\n",
    "    data_f.seek(0)\n",
    "\n",
    "    return input_size, num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][2017-05-26 16:42:04] The following columns are filtered: Record ID, Agency Name, Agency Code, Year, Month, Record Source\n",
      "[INFO][2017-05-26 16:42:15] The number of distinct values for each column are:\n",
      "\n",
      "[INFO][2017-05-26 16:42:15]   Agency Type = 15\n",
      "[INFO][2017-05-26 16:42:15]   City = 1784\n",
      "[INFO][2017-05-26 16:42:15]   State = 59\n",
      "[INFO][2017-05-26 16:42:15]   Incident = 1012\n",
      "[INFO][2017-05-26 16:42:15]   Crime Type = 20\n",
      "[INFO][2017-05-26 16:42:15]   Crime Solved = 4\n",
      "[INFO][2017-05-26 16:42:15]   Victim Sex = 5\n",
      "[INFO][2017-05-26 16:42:15]   Victim Age = 104\n",
      "[INFO][2017-05-26 16:42:15]   Victim Race = 58\n",
      "[INFO][2017-05-26 16:42:15]   Victim Ethnicity = 7\n",
      "[INFO][2017-05-26 16:42:15]   Perpetrator Sex = 4\n",
      "[INFO][2017-05-26 16:42:15]   Perpetrator Age = 104\n",
      "[INFO][2017-05-26 16:42:15]   Perpetrator Race = 42\n",
      "[INFO][2017-05-26 16:42:15]   Perpetrator Ethnicity = 6\n",
      "[INFO][2017-05-26 16:42:15]   Relationship = 29\n",
      "[INFO][2017-05-26 16:42:15]   Weapon = 27\n",
      "[INFO][2017-05-26 16:42:15]   Victim Count = 22\n",
      "[INFO][2017-05-26 16:42:15]   Perpetrator Count = 11\n",
      "[INFO][2017-05-26 16:42:15] \n",
      "The generated vectors will have a total of 3313 entries each\n",
      "[INFO][2017-05-26 16:42:15] The dataset has 638454 samples\n",
      "[INFO][2017-05-26 16:42:20] Processed 100000 samples (15.7%)...\n",
      "[INFO][2017-05-26 16:42:20] Storing collected data in CSV file...\n"
     ]
    }
   ],
   "source": [
    "with open(RAW_DATA_PATH, 'r') as data_f:\n",
    "    with open(PREPROCESSED_DATA_PATH, 'w+') as out_f:\n",
    "        # Read headings\n",
    "        columns = data_f.readline().strip('\\n').split(',')\n",
    "        col_values = defaultdict(list)\n",
    "        last_idx = 0\n",
    "\n",
    "        log('The following columns are filtered: %s' % ', '.join(STRIP_COLS))\n",
    "\n",
    "        # Find all unique values for each row in the dataset\n",
    "        # and store them in col_values.\n",
    "        for i, line in enumerate(data_f):\n",
    "            sample_values = line.strip('\\n').split(',')\n",
    "\n",
    "            for c, v in zip(columns, sample_values):\n",
    "                if c in STRIP_COLS: continue\n",
    "                if v not in col_values[c]: col_values[c].append(v)\n",
    "\n",
    "            last_idx = i\n",
    "\n",
    "        log('The number of distinct values for each column are:\\n')\n",
    "\n",
    "        sum_lines = last_idx+1\n",
    "        sum_vec_entries = 0\n",
    "\n",
    "        for c, v in col_values.items():\n",
    "            log('  %s = %d' % (c, len(v)))\n",
    "            sum_vec_entries += len(v)\n",
    "\n",
    "        log('The generated vectors will have a total of %d entries each' % sum_vec_entries)\n",
    "        log('The dataset has %i samples' % sum_lines)\n",
    "\n",
    "        data_f.seek(0)\n",
    "        data_f.readline() # skip headings after seek(0)\n",
    "\n",
    "        start_time = time.time()\n",
    "        curr_idx = 0\n",
    "        temp_x = []\n",
    "\n",
    "        for i, line in enumerate(data_f):\n",
    "            sample_values = line.strip('\\n').split(',')\n",
    "            sample_vec = np.zeros(sum_vec_entries)\n",
    "            idx_offset = 0\n",
    "\n",
    "            for c, v in zip(columns, sample_values):\n",
    "                if c in STRIP_COLS: continue\n",
    "                sample_vec[col_values[c].index(v)+idx_offset] = 1\n",
    "                idx_offset += len(col_values[c])\n",
    "\n",
    "            temp_x.append(sample_vec)\n",
    "\n",
    "            if (i+1) % 100000 == 0 or (i+1) == sum_lines:\n",
    "                temp_x = np.array(temp_x)\n",
    "                np.random.shuffle(temp_x)\n",
    "\n",
    "                log('Processed %i samples (%.1f%%)...' % (i+1, 100*(float(i+1)/sum_lines)))\n",
    "                log('Storing collected data in CSV file...')\n",
    "\n",
    "                temp_x_str = []\n",
    "\n",
    "                for i in range(temp_x.shape[0]):\n",
    "                    temp_x_str.append(';'.join(map(str, map(int, temp_x[i]))))\n",
    "           \n",
    "                out_f.write('%s\\n' % '\\n'.join(temp_x_str))\n",
    "\n",
    "                curr_idx += temp_x.shape[0]\n",
    "\n",
    "                log('Stored data successfully! (Took %.2fs)' % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                temp_x = []\n",
    "\n",
    "        log('Successfully stored preprocessed samples in: %s' % PREPROCESSED_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Skript konvertiert alle Morde in _database.csv_ in One-Hot Encoded Vektoren und speichert diese in einer CSV Datei. Diese können dann verwendet werden um damit den _AutEncoder_ zu trainieren und danach Embeddings für jeden einzelnen Mord zu generieren.\n",
    "\n",
    "Beim Preprocessen werden alle Spalten berücksichtigt ausser diejenigen, welche in der Liste STRIP_COLS explizit ausgeschlossen werden. Das ganz funktioniert so, dass zuerst für jede Spalte eruiert wird, wieviele unterschiedliche Werte es pro Spalte hat. Wenn für eine Spalte _n_ verschiedene Werte vorhanden sind, so werden für die Darstellung im One-Hot Vektor entsprechend _n_ Werte für diese Spalte benötigt. Die resultierenden Vektoren sind also \n",
    "\n",
    "$$\\sum_{c \\in Columns} \\operatorname{classes}(c)$$\n",
    "\n",
    "lang, wobei _C_ für die Menge aller Spalten und _classes_ für die Anzahl unterschiedlicher Werte für die Spalte _c_ steht. Pro Zeile und Spalte werden dann diejenigen Werte, welche in der jeweiligen Zeile stehen auf _1_ gesetzt, alle anderen werden auf _0_ belassen. Als nächste folgt der Code in [TensorFlow](https://www.tensorflow.org/), welcher für die Implementation des _AutoEncoder_ und des _VariationalAutoEncoder_ zuständig ist.\n",
    "\n",
    "Prinzipell sind _AutoEncoder_ eine spezielle Art von Neuronalen Netzen (NN), welche dafür zuständig sind, eine effiziente Codierung der Eingabedaten zu lernen. Der Aufbau ist so, dass die Eingabedaten als One-Hote Encoded Vektoren (im Bild unten _x_) über den Input Layer in das NN eingespeist wird. Diese werden dann mithilfe einer Multiplikation mit einer Gewichts-Matrix in den Hidden Layer projeziert. Dieser ist im Falle von AutoEncodern **immer** kleiner wie der Input Layer, weil das NN ja eine effiziente Codierung der Eingabedaten lernen soll. Am Ende wird das NN mithilfe von Gradient-Descent mit dem _Adam_ Optimierer darauf trainiert, aus der codierten Darstellung der Eingabedaten (im Bild unten _z_) wieder die Eingabedaten _x_ zu rekonstruieren. Die generierten Darstellungen _z_ können dann als Embeddings der Eingabedaten in einem _m_-dimensionalen Vektorraum aufgefasst werden, wobei _m_ der grösse des Hidden Layer in der Mitte entspricht. Auf diese Embeddings können wir dann später Clustering-Algorithmen anwenden um festzustellen, welche Verbrechen im eingebetteten Vektor-Raum nahe beieinander liegen.\n",
    "\n",
    "![AutoEncoder Struktur](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n",
    "\n",
    "Der Unterschied eines \"normalen\" _AutoEncoder_ zu einem _VariationalAutoEncoder_ liegt darin, dass ...\n",
    "Für eine gute Einführung in _VariationalAutoEncoder_ kann [dieses](http://kvfrans.com/variational-autoencoders-explained/) Tutorial hinzugezogen werden.\n",
    "\n",
    "Wir werden alle Experimente mit beiden Varianten _AutoEncoder_ und _VariationalAutoEncoder_ durchführen. Unten folgt die Defintion der Modelle in Python mithilfe von TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The current version of the autoencoder to use\n",
    "CurrentAutoEncoder = AutoEncoder\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, input_size, hidden_size, session):\n",
    "        '''Initializes a new instance of the VariationalAutoencoder class.'''\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {}\n",
    "        self.transfer_fn = tf.nn.softplus\n",
    "\n",
    "        self.__initialize()\n",
    "        self.__build()\n",
    "\n",
    "    def __initialize(self):\n",
    "        '''Initializes the weights needed to build the computational graph.'''\n",
    "        weights = {}\n",
    "\n",
    "        weights['weights_1'] = tf.get_variable('weights_1',[self.input_size, self.hidden_size])\n",
    "        weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['weights_2'] = tf.Variable(tf.zeros([self.hidden_size, self.input_size], dtype=tf.float32))\n",
    "        weights['bias_2'] = tf.Variable(tf.zeros([self.input_size], dtype=tf.float32))\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''Returns the optimizer for this instance.'''\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_loss(self):\n",
    "        '''Returns the loss function for this instance.'''\n",
    "        return self.loss_fn\n",
    "\n",
    "    def get_weights_and_biases(self):\n",
    "        '''Returns the weights and biases of this instance.'''\n",
    "        return self.weights\n",
    "\n",
    "    def get_internal_representation(self):\n",
    "        '''Returns the internal, embedded representation variables.'''\n",
    "        return self.hidden\n",
    "\n",
    "    def batch_fit(self, input):\n",
    "        '''Fits the model to the given batch.'''\n",
    "        loss, _ = self.session.run((self.loss_fn, self.optimizer),\n",
    "                                   feed_dict={self.input: input})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, input):\n",
    "        return self.session.run(self.hidden, feed_dict={self.input: input})\n",
    "\n",
    "    def __build(self):\n",
    "        '''Builds the computational graph.'''\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        hidden_1_result = tf.matmul(self.input, self.weights['weights_1'])\n",
    "        self.hidden = self.transfer_fn(tf.add(hidden_1_result,\n",
    "                                              self.weights['bias_1']))\n",
    "\n",
    "        reconstruction_result = tf.matmul(self.hidden, self.weights['weights_2'])\n",
    "        self.reconstruction = tf.add(reconstruction_result, self.weights['bias_2'])\n",
    "\n",
    "        diff = tf.subtract(self.reconstruction, self.input)\n",
    "        self.loss_fn = 0.5 * tf.reduce_sum(tf.pow(diff, 2.0))\n",
    "        self.optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.optimizer = self.optimizer_fn.minimize(self.loss_fn)\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, input_size, hidden_size, session):\n",
    "        '''Initializes a new instance of the VariationalAutoencoder class.'''\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {}\n",
    "\n",
    "        self.__initialize()\n",
    "        self.__build()\n",
    "\n",
    "    def __initialize(self):\n",
    "        '''Initializes the weights needed to build the computational graph.'''\n",
    "        weights = {}\n",
    "\n",
    "        weights['weights_1'] = tf.get_variable('weights_1',[self.input_size, self.hidden_size])\n",
    "        weights['log_sigma_weights_1'] = tf.get_variable('log_sigma_weights_1', [self.input_size, self.hidden_size])\n",
    "        weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['log_sigma_bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['weights_2'] = tf.Variable(tf.zeros([self.hidden_size, self.input_size], dtype=tf.float32))\n",
    "        weights['bias_2'] = tf.Variable(tf.zeros([self.input_size], dtype=tf.float32))\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''Returns the optimizer for this instance.'''\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_loss(self):\n",
    "        '''Returns the loss function for this instance.'''\n",
    "        return self.loss_fn\n",
    "\n",
    "    def get_weights(self):\n",
    "        '''Returns the weights of this instance.'''\n",
    "        return self.weights\n",
    "\n",
    "    def get_internal_representation(self):\n",
    "        '''Returns the internal, embedded representation variables.'''\n",
    "        return self.z\n",
    "\n",
    "    def batch_fit(self, input):\n",
    "        '''Fits the model to the given batch.'''\n",
    "        loss, _ = self.session.run((self.loss_fn, self.optimizer),\n",
    "                                   feed_dict={self.input: input})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, input):\n",
    "        return self.session.run(self.z_mean, feed_dict={self.input: input})\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        loss, opt = self.sess.run((self.loss_fn, self.optimizer), feed_dict={self.x: X})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "\n",
    "    def __build(self):\n",
    "        '''Builds the computational graph.'''\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        hidden_1_result = tf.matmul(self.input, self.weights['weights_1'])\n",
    "        self.z_mean = tf.add(hidden_1_result, self.weights['log_sigma_bias_1'])\n",
    "\n",
    "        log_sigma_result = tf.matmul(self.input, self.weights['log_sigma_weights_1'])\n",
    "        self.z_log_sigma_sq = tf.add(log_sigma_result, self.weights['log_sigma_bias_1'])\n",
    "\n",
    "        eps = tf.random_normal(tf.stack([tf.shape(self.input)[0], self.hidden_size]), 0, 1, dtype=tf.float32)\n",
    "        self.z = tf.add(self.z_mean, tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        y_result = tf.matmul(self.z, self.weights['weights_2'])\n",
    "        self.y = tf.add(y_result, self.weights['bias_2'])\n",
    "\n",
    "        reconstruction_loss = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.y, self.input), 2.0))\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \\\n",
    "                                           - tf.square(self.z_mean) \\\n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "        self.loss_fn = tf.reduce_mean(reconstruction_loss + latent_loss)\n",
    "        self.optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.optimizer = self.optimizer_fn.minimize(self.loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Definition der Modelle folgt der Code um diese zu trainieren. Das Training durchzuführen dauert entsprechend lange, auf einer mittelmässigen GPU benötigt es ca. einen Tag Rechenzeit. Im folgenden Abschnitt ist der Code, welcher zuständig ist für das Training des _AutoEncoder_, ersichtlich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_type = camel_to_sneak(CurrentAutoEncoder.__name__)\n",
    "time_stamp = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())\n",
    "result_name = '%s-%s-results/' % (time_stamp, encoder_type)\n",
    "result_path = path.join(RESULTS_DIR, result_name)\n",
    "loss_track = []\n",
    "\n",
    "with open(PREPROCESSED_DATA_PATH, 'r') as train_f:\n",
    "    input_size, num_samples = get_input_size_and_length(train_f)\n",
    "\n",
    "    log('Starting training with a %s' % CurrentAutoEncoder.__name__)\n",
    "\n",
    "    autoencoder = CurrentAutoEncoder(input_size, HIDDEN_SIZE, session=session)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        log('Starting epoch #%d' % (epoch+1))\n",
    "        num_batches = int(num_samples / BATCH_SIZE)\n",
    "        avg_loss = 0\n",
    "\n",
    "        for num_batch in range(num_batches):\n",
    "            batch_x = get_next_batch(train_f, BATCH_SIZE)\n",
    "            loss = autoencoder.batch_fit(batch_x)\n",
    "            avg_loss += (loss / num_samples) * BATCH_SIZE\n",
    "\n",
    "            if (num_batch+1) % DISPLAY_BATCH == 0 or (num_batches-num_batch) < 5:\n",
    "                log('Batch #%d of #%d, loss = %.5f' % (num_batch+1, num_batches, loss))\n",
    "\n",
    "        if (epoch+1) % DISPLAY_EPOCH == 0 or (epoch+1) == TRAINING_EPOCHS:\n",
    "            log('Epoch #%d of #%d, loss = %.5f' % (epoch+1, TRAINING_EPOCHS, avg_loss))\n",
    "            saver.save(session, result_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem der _AutoEncoder_ trainiert wurde kann dieser verwendet werden um damit die Embeddings für die einzelnen Verbrechen zu generieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PREPROCESSED_DATA_PATH, 'r') as samples_f:\n",
    "    with open(emb_out_path, 'w+') as emb_f:\n",
    "        input_size, num_samples = get_input_size_and_length(train_f)\n",
    "        log('Restoring model from %s' % model_path)\n",
    "\n",
    "        autoencoder = CurrentAutoEncoder(input_size, HIDDEN_SIZE, session=session)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "        saver.restore(session, model_path)\n",
    "\n",
    "        log('Finished restoring the model')\n",
    "        log('Starting to embed the samples in %s' % samples_path)\n",
    "\n",
    "        num_batches = int(num_samples / BATCH_SIZE)\n",
    "\n",
    "        for num_batch in range(num_batches):\n",
    "            batch_x = get_next_batch(samples_data, num_batch)\n",
    "            batch_y = autoencoder.transform(batch_x)\n",
    "\n",
    "            for y in batch_y:\n",
    "                emb_f.write('%s\\n' % ';'.join(map(str, y)))\n",
    "\n",
    "            if (num_batch+1) % DISPLAY_BATCH == 0:\n",
    "                log('Processed %d of %d samples' % (num_batch+1, num_batches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun, da wir die Embeddings für alle Verbrechen generiert haben, können wir damit starten das K-Means Clustering auf diese anzuwenden, als auch auf die ursprünglichen One-Hot Encoded Vektoren, welche verwendet wurden um den _AutoEncoder_ zu trainieren:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
