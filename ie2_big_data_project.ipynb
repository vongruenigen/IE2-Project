{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IE2 Big Data Project (weilemar & vongrdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Jupyter Notebook wird der Code vorgestellt, welcher verwendet wurde um das [Homocide Report](https://www.kaggle.com/murderaccountability/homicide-reports) Dataset zu analysieren. Ziel dabei ist es ...\n",
    "\n",
    "Als erster Schritt muss das Dataset über die folgende URL heruntergeladen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -O homocide-reports.zip https://github.com/vongruenigen/IE2-Project/raw/master/data/homicide-reports.zip\n",
    "unzip homocide-reports.zip\n",
    "\n",
    "if [ ! -f database.csv ]\n",
    "then\n",
    "    echo 'ERROR: Could not fetch or extract homocide database!'\n",
    "    exit 2\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Dataset heruntergeladen und entpackt ist muss daraus mittels des preprocessing Skripts (für Original siehe _scripts/preprocess_data.py_) das eigentliche Dataset mit One-Hot Encoded Vektoren verwendet werden. Da dieses Skript hier im Jupyter-Notebook nicht direkt aufgerufen werden kann müssen die Parameter in der Variable _argv_ gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from os import path\n",
    "\n",
    "argv = ['database.csv', 'samples.h5']\n",
    "\n",
    "if len(argv) < 2:\n",
    "    print('ERROR: process_data.py <data-csv> <out-h5>')\n",
    "    sys.exit(2)\n",
    "\n",
    "data_path = argv[0]\n",
    "out_path = argv[1]\n",
    "\n",
    "# Columns we are going to ignore in the dataset\n",
    "# because they're redundant or non-informative\n",
    "STRIP_COLS = ('Record ID', 'Agency Name', 'Agency Code',\n",
    "              'Year', 'Month', 'Record Source')\n",
    "\n",
    "if path.isfile(out_path):\n",
    "    os.remove(out_path)\n",
    "\n",
    "with open(data_path, 'r') as data_f:\n",
    "    with h5py.File(out_path) as out_f:\n",
    "        # Read headings\n",
    "        columns = data_f.readline().strip('\\n').split(',')\n",
    "        col_values = defaultdict(list)\n",
    "        last_idx = 0\n",
    "\n",
    "        print('The following columns are filtered: %s' % ', '.join(STRIP_COLS))\n",
    "\n",
    "        # Find all unique values for each row in the dataset\n",
    "        # and store them in col_values.\n",
    "        for i, line in enumerate(data_f):\n",
    "            sample_values = line.strip('\\n').split(',')\n",
    "\n",
    "            for c, v in zip(columns, sample_values):\n",
    "                if c in STRIP_COLS: continue\n",
    "                if v not in col_values[c]: col_values[c].append(v)\n",
    "\n",
    "            last_idx = i\n",
    "\n",
    "        print('The number of distinct values for each column are:\\n')\n",
    "\n",
    "        sum_lines = last_idx + 1\n",
    "        sum_vec_entries = 0\n",
    "\n",
    "        for c, v in col_values.items():\n",
    "            print('  %s = %d' % (c, len(v)))\n",
    "            sum_vec_entries += len(v)\n",
    "\n",
    "        print('\\nThe generated vectors will have a total of %d entries each' % sum_vec_entries)\n",
    "        print('The dataset has %i samples\\n' % sum_lines)\n",
    "\n",
    "        data_f.seek(0)\n",
    "        data_f.readline() # skip headings after seek(0)\n",
    "\n",
    "        X = out_f.create_dataset('x', dtype='i8',\n",
    "                                 shape=(sum_lines, sum_vec_entries))\n",
    "\n",
    "        start_time = time.time()\n",
    "        curr_idx = 0\n",
    "        temp_x = []\n",
    "\n",
    "        for i, line in enumerate(data_f):\n",
    "            sample_values = line.strip('\\n').split(',')\n",
    "            sample_vec = np.zeros(sum_vec_entries)\n",
    "            idx_offset = 0\n",
    "\n",
    "            for c, v in zip(columns, sample_values):\n",
    "                if c in STRIP_COLS: continue\n",
    "                sample_vec[col_values[c].index(v)+idx_offset] = 1\n",
    "                idx_offset += len(col_values[c])\n",
    "\n",
    "            temp_x.append(sample_vec)\n",
    "\n",
    "            if (i+1) % 100000 == 0 or (i+1) == sum_lines:\n",
    "                temp_x = np.array(temp_x)\n",
    "                np.random.shuffle(temp_x)\n",
    "\n",
    "                print('Processed %i samples (%.1f%%)...' % (i+1, 100*(float(i+1)/sum_lines)))\n",
    "                print('Storing collected data in h5py file...')\n",
    "\n",
    "                X[curr_idx:curr_idx+temp_x.shape[0]] = temp_x\n",
    "                curr_idx += temp_x.shape[0]\n",
    "\n",
    "                print('\\nStored data successfully! (Took %.2fs)' % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                temp_x = []\n",
    "\n",
    "        print('Successfully stored preprocessed samples in: %s' % out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Skript konvertiert alle Morde in _database.csv_ in One-Hot Encoded Vektoren und speichert diese in einer HDF5 Datei. Diese können dann verwendet werden um damit den _AutEncoder_ zu trainieren und danach Embeddings für jeden einzelnen Mord zu generieren.\n",
    "\n",
    "Beim Preprocessen werden alle Spalten berücksichtigt ausser diejenigen, welche in der Liste STRIP_COLS explizit ausgeschlossen werden. Das ganz funktioniert so, dass zuerst für jede Spalte eruiert wird, wieviele unterschiedliche Werte es pro Spalte hat. Wenn für eine Spalte _n_ verschiedene Werte vorhanden sind, so werden für die Darstellung im One-Hot Vektor entsprechend _n_ Werte für diese Spalte benötigt. Die resultierenden Vektoren sind also \n",
    "\n",
    "$$\\sum_{c \\in Columns} \\operatorname{classes}(c)$$\n",
    "\n",
    "lang, wobei _C_ für die Menge aller Spalten und _classes_ für die Anzahl unterschiedlicher Werte für die Spalte _c_ steht. Pro Zeile und Spalte werden dann diejenigen Werte, welche in der jeweiligen Zeile stehen auf _1_ gesetzt, alle anderen werden auf _0_ belassen. Als nächste folgt der Code in [TensorFlow](https://www.tensorflow.org/), welcher für die Implementation des _AutoEncoder_ und des _VariationalAutoEncoder_ zuständig ist.\n",
    "\n",
    "Prinzipell sind _AutoEncoder_ eine spezielle Art von Neuronalen Netzen (NN), welche dafür zuständig sind, eine effiziente Codierung der Eingabedaten zu lernen. Der Aufbau ist so, dass die Eingabedaten als One-Hote Encoded Vektoren (im Bild unten _x_) über den Input Layer in das NN eingespeist wird. Diese werden dann mithilfe einer Multiplikation mit einer Gewichts-Matrix in den Hidden Layer projeziert. Dieser ist im Falle von AutoEncodern **immer** kleiner wie der Input Layer, weil das NN ja eine effiziente Codierung der Eingabedaten lernen soll. Am Ende wird das NN mithilfe von Gradient-Descent mit dem _Adam_ Optimierer darauf trainiert, aus der codierten Darstellung der Eingabedaten (im Bild unten _z_) wieder die Eingabedaten _x_ zu rekonstruieren. Die generierten Darstellungen _z_ können dann als Embeddings der Eingabedaten in einem _m_-dimensionalen Vektorraum aufgefasst werden, wobei _m_ der grösse des Hidden Layer in der Mitte entspricht. Auf diese Embeddings können wir dann später Clustering-Algorithmen anwenden um festzustellen, welche Verbrechen im eingebetteten Vektor-Raum nahe beieinander liegen.\n",
    "\n",
    "![AutoEncoder Struktur](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n",
    "\n",
    "Der Unterschied eines \"normalen\" _AutoEncoder_ zu einem _VariationalAutoEncoder_ liegt darin, dass ...\n",
    "Für eine gute Einführung in _VariationalAutoEncoder_ kann [dieses](http://kvfrans.com/variational-autoencoders-explained/) Tutorial hinzugezogen werden.\n",
    "\n",
    "Wir werden alle Experimente mit beiden Varianten _AutoEncoder_ und _VariationalAutoEncoder_ durchführen. Unten folgt die Defintion der Modelle in Python mithilfe von TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, input_size, hidden_size, session):\n",
    "        '''Initializes a new instance of the VariationalAutoencoder class.'''\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {}\n",
    "        self.transfer_fn = tf.nn.softplus\n",
    "\n",
    "        self.__initialize()\n",
    "        self.__build()\n",
    "\n",
    "    def __initialize(self):\n",
    "        '''Initializes the weights needed to build the computational graph.'''\n",
    "        weights = {}\n",
    "\n",
    "        weights['weights_1'] = tf.get_variable('weights_1',[self.input_size, self.hidden_size])\n",
    "        weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['weights_2'] = tf.Variable(tf.zeros([self.hidden_size, self.input_size], dtype=tf.float32))\n",
    "        weights['bias_2'] = tf.Variable(tf.zeros([self.input_size], dtype=tf.float32))\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''Returns the optimizer for this instance.'''\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_loss(self):\n",
    "        '''Returns the loss function for this instance.'''\n",
    "        return self.loss_fn\n",
    "\n",
    "    def get_weights_and_biases(self):\n",
    "        '''Returns the weights and biases of this instance.'''\n",
    "        return self.weights\n",
    "\n",
    "    def get_internal_representation(self):\n",
    "        '''Returns the internal, embedded representation variables.'''\n",
    "        return self.hidden\n",
    "\n",
    "    def batch_fit(self, input):\n",
    "        '''Fits the model to the given batch.'''\n",
    "        loss, _ = self.session.run((self.loss_fn, self.optimizer),\n",
    "                                   feed_dict={self.input: input})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, input):\n",
    "        return self.session.run(self.hidden, feed_dict={self.input: input})\n",
    "\n",
    "    def __build(self):\n",
    "        '''Builds the computational graph.'''\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        hidden_1_result = tf.matmul(self.input, self.weights['weights_1'])\n",
    "        self.hidden = self.transfer_fn(tf.add(hidden_1_result,\n",
    "                                              self.weights['bias_1']))\n",
    "\n",
    "        reconstruction_result = tf.matmul(self.hidden, self.weights['weights_2'])\n",
    "        self.reconstruction = tf.add(reconstruction_result, self.weights['bias_2'])\n",
    "\n",
    "        diff = tf.subtract(self.reconstruction, self.input)\n",
    "        self.loss_fn = 0.5 * tf.reduce_sum(tf.pow(diff, 2.0))\n",
    "        self.optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.optimizer = self.optimizer_fn.minimize(self.loss_fn)\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, input_size, hidden_size, session):\n",
    "        '''Initializes a new instance of the VariationalAutoencoder class.'''\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {}\n",
    "\n",
    "        self.__initialize()\n",
    "        self.__build()\n",
    "\n",
    "    def __initialize(self):\n",
    "        '''Initializes the weights needed to build the computational graph.'''\n",
    "        weights = {}\n",
    "\n",
    "        weights['weights_1'] = tf.get_variable('weights_1',[self.input_size, self.hidden_size])\n",
    "        weights['log_sigma_weights_1'] = tf.get_variable('log_sigma_weights_1', [self.input_size, self.hidden_size])\n",
    "        weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['log_sigma_bias_1'] = tf.Variable(tf.zeros([self.hidden_size], dtype=tf.float32))\n",
    "        weights['weights_2'] = tf.Variable(tf.zeros([self.hidden_size, self.input_size], dtype=tf.float32))\n",
    "        weights['bias_2'] = tf.Variable(tf.zeros([self.input_size], dtype=tf.float32))\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''Returns the optimizer for this instance.'''\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_loss(self):\n",
    "        '''Returns the loss function for this instance.'''\n",
    "        return self.loss_fn\n",
    "\n",
    "    def get_weights(self):\n",
    "        '''Returns the weights of this instance.'''\n",
    "        return self.weights\n",
    "\n",
    "    def get_internal_representation(self):\n",
    "        '''Returns the internal, embedded representation variables.'''\n",
    "        return self.z\n",
    "\n",
    "    def batch_fit(self, input):\n",
    "        '''Fits the model to the given batch.'''\n",
    "        loss, _ = self.session.run((self.loss_fn, self.optimizer),\n",
    "                                   feed_dict={self.input: input})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, input):\n",
    "        return self.session.run(self.z_mean, feed_dict={self.input: input})\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        loss, opt = self.sess.run((self.loss_fn, self.optimizer), feed_dict={self.x: X})\n",
    "        return loss\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "\n",
    "    def __build(self):\n",
    "        '''Builds the computational graph.'''\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        hidden_1_result = tf.matmul(self.input, self.weights['weights_1'])\n",
    "        self.z_mean = tf.add(hidden_1_result, self.weights['log_sigma_bias_1'])\n",
    "\n",
    "        log_sigma_result = tf.matmul(self.input, self.weights['log_sigma_weights_1'])\n",
    "        self.z_log_sigma_sq = tf.add(log_sigma_result, self.weights['log_sigma_bias_1'])\n",
    "\n",
    "        eps = tf.random_normal(tf.stack([tf.shape(self.input)[0], self.hidden_size]), 0, 1, dtype=tf.float32)\n",
    "        self.z = tf.add(self.z_mean, tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        y_result = tf.matmul(self.z, self.weights['weights_2'])\n",
    "        self.y = tf.add(y_result, self.weights['bias_2'])\n",
    "\n",
    "        reconstruction_loss = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.y, self.input), 2.0))\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \\\n",
    "                                           - tf.square(self.z_mean) \\\n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "        self.loss_fn = tf.reduce_mean(reconstruction_loss + latent_loss)\n",
    "        self.optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.optimizer = self.optimizer_fn.minimize(self.loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Definition der Modelle folgt der Code um diese zu trainieren. Das Training durchzuführen dauert entsprechend lange, auf einer mittelmässigen GPU benötigt es ca. einen Tag Rechenzeit. Als erstes werden alle benötigten Bibliotheken importiert und einige Hilfs-Funktionen definiert bevor der eigentliche Code für das Training folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "from os import path\n",
    "from numpy import random\n",
    "\n",
    "def camel_to_sneak(name):\n",
    "    '''Convert a string from camel-case to sneak-case.'''\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def error(msg):\n",
    "    log(msg, level='error')\n",
    "\n",
    "def log(msg, level='info'):\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    print('[%s][%s] %s' % (level.upper(), ts, msg))\n",
    "    if level == 'error': sys.exit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "TRAINING_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "DISPLAY_EPOCH = 1\n",
    "DISPLAY_BATCH = 1000\n",
    "HIDDEN_SIZE = 256\n",
    "RESULTS_DIR = path.abspath(path.join(path.dirname(__file__), 'results'))\n",
    "\n",
    "CurrentAutoEncoder = AutoEncoder\n",
    "\n",
    "def camel_to_sneak(name):\n",
    "    '''Convert a string from camel-case to sneak-case.'''\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def error(msg):\n",
    "    log(msg, level='error')\n",
    "\n",
    "def log(msg, level='info'):\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    print('[%s][%s] %s' % (level.upper(), ts, msg))\n",
    "    if level == 'error': sys.exit(2)\n",
    "\n",
    "encoder_type = camel_to_sneak(CurrentAutoEncoder.__name__)\n",
    "time_stamp = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())\n",
    "result_name = '%s-%s-results/' % (time_stamp, encoder_type)\n",
    "result_path = path.join(RESULTS_DIR, result_name)\n",
    "\n",
    "def get_next_batch(train_data, num_batch, random=False, available_idxs=None):\n",
    "    '''Helper function for retrieving the next batch from train_data.'''\n",
    "    idxs = None\n",
    "\n",
    "    if random:\n",
    "        idxs = [available_idxs.pop(random.randint(0, len(available_idxs))) \\\n",
    "                for _ in range(BATCH_SIZE)]\n",
    "        idxs = list(sorted(idxs))\n",
    "    else:\n",
    "        start_idx = num_batch*BATCH_SIZE\n",
    "        idxs = range(start_idx, start_idx+BATCH_SIZE)\n",
    "\n",
    "    batch_data = train_data[idxs]\n",
    "\n",
    "    if random:\n",
    "        return (batch_data, available_idxs)\n",
    "    else:\n",
    "        return batch_data\n",
    "\n",
    "train_data_path = argv[0]\n",
    "loss_track = []\n",
    "\n",
    "with h5py.File(train_data_path) as train_f:\n",
    "    train_data = train_f['x']\n",
    "    input_size = train_data.shape[1]\n",
    "    num_samples = train_data.shape[0]\n",
    "\n",
    "    log('Starting training with a %s' % CurrentAutoEncoder.__name__)\n",
    "\n",
    "    autoencoder = CurrentAutoEncoder(input_size, HIDDEN_SIZE, session=session)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        log('Starting epoch #%d' % (epoch+1))\n",
    "        num_batches = int(num_samples / BATCH_SIZE)\n",
    "        avg_loss = 0\n",
    "\n",
    "        for num_batch in range(num_batches):\n",
    "            batch_x = get_next_batch(train_data, num_batch)\n",
    "            loss = autoencoder.batch_fit(batch_x)\n",
    "            avg_loss += (loss / num_samples) * BATCH_SIZE\n",
    "\n",
    "            if (num_batch+1) % DISPLAY_BATCH == 0 or (num_batches-num_batch) < 5:\n",
    "                log('Batch #%d of #%d, loss = %.5f' % (num_batch+1, num_batches, loss))\n",
    "\n",
    "        if (epoch+1) % DISPLAY_EPOCH == 0 or (epoch+1) == TRAINING_EPOCHS:\n",
    "            log('Epoch #%d of #%d, loss = %.5f' % (epoch+1, TRAINING_EPOCHS, avg_loss))\n",
    "            saver.save(session, result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem der _AutoEncoder_ trainiert wurde kann dieser verwendet werden um damit die Embeddings für die einzelnen Verbrechen zu generieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
